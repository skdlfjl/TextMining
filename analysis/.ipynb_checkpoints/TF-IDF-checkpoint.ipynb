{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 분석2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 각 카테고리의 데이터 개수(채용공고의 개수)를 가져와서 튜플형태로 저장한 뒤, 리스트에 넣는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 746), (1, 683), (2, 624), (3, 494), (4, 287), (5, 274), (6, 256), (7, 233), (8, 220), (9, 193), (10, 163), (11, 148), (12, 168), (13, 141), (14, 171), (15, 162), (16, 137), (17, 109)]\n"
     ]
    }
   ],
   "source": [
    "category_len = []\n",
    "\n",
    "for category_num in range(18):\n",
    "    data = open('../crawling_all_1211/category{0}.txt'.format(category_num), 'r', encoding = 'utf-8')\n",
    "    lines = data.readlines()  # 데이터 불러와서 line 단위로 넣는다\n",
    "    #print(len(lines))\n",
    "    tu = category_num, len(lines)\n",
    "    category_len.append(tu)\n",
    "    \n",
    "print(category_len)# ( 카테고리번호, 크롤링 데이터 개수 ) 모양의 튜플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 총 18개의 크롤링 데이터(클렌징까지 마침)를 불러와서, category_sum 리스트에 넣어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_num, data_num in category_len:\n",
    "    \n",
    "    data = open('../cleansing_all_1211/data_cleansing_all/cleansing_category{0}_list{1}.txt'.format(category_num, data_num), 'r', encoding = 'utf-8')\n",
    "    data = data.readlines()  # 데이터 불러와서 line 단위로 넣는다\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].replace('\\n', '')    # 각 문장마다 들어있는 줄바꿈 기호 없애주기\n",
    "    \n",
    "    data = ' '.join(data)\n",
    "    \n",
    "    ## 각 변수에 저장하자\n",
    "    globals()['category{}'.format(category_num)] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sum = []\n",
    "\n",
    "for category_num in range(18):\n",
    "    category = globals()['category{}'.format(category_num)]\n",
    "    category_sum.append(category)\n",
    "    \n",
    "len(category_sum) # 총 18개의 크롤링 데이터를 차례대로 category_sum 리스트에 넣어줬다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocabulary_all_1211 디렉토리 안의 tokens{}_.txt를 가져옵니다 (전처리가 완료된 토큰)\n",
    "\n",
    "# tokens{}: {}카테고리의 전처리 완료된 전체 토큰\n",
    "# voca{}: {}카테고리의 유니트 토큰\n",
    "\n",
    "# common_voca: 전체 데이터(1211) 공통등장 voca(토큰)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 번째 category (tokens): 39330\n",
      "# voca size: 1218 \n",
      "\n",
      "# 1 번째 category (tokens): 32265\n",
      "# voca size: 1121 \n",
      "\n",
      "# 2 번째 category (tokens): 31409\n",
      "# voca size: 1083 \n",
      "\n",
      "# 3 번째 category (tokens): 23563\n",
      "# voca size: 885 \n",
      "\n",
      "# 4 번째 category (tokens): 12172\n",
      "# voca size: 589 \n",
      "\n",
      "# 5 번째 category (tokens): 11449\n",
      "# voca size: 552 \n",
      "\n",
      "# 6 번째 category (tokens): 11239\n",
      "# voca size: 565 \n",
      "\n",
      "# 7 번째 category (tokens): 10772\n",
      "# voca size: 536 \n",
      "\n",
      "# 8 번째 category (tokens): 9931\n",
      "# voca size: 545 \n",
      "\n",
      "# 9 번째 category (tokens): 8665\n",
      "# voca size: 451 \n",
      "\n",
      "# 10 번째 category (tokens): 7375\n",
      "# voca size: 406 \n",
      "\n",
      "# 11 번째 category (tokens): 5730\n",
      "# voca size: 333 \n",
      "\n",
      "# 12 번째 category (tokens): 5838\n",
      "# voca size: 393 \n",
      "\n",
      "# 13 번째 category (tokens): 5838\n",
      "# voca size: 418 \n",
      "\n",
      "# 14 번째 category (tokens): 8058\n",
      "# voca size: 505 \n",
      "\n",
      "# 15 번째 category (tokens): 7181\n",
      "# voca size: 389 \n",
      "\n",
      "# 16 번째 category (tokens): 5625\n",
      "# voca size: 330 \n",
      "\n",
      "# 17 번째 category (tokens): 4639\n",
      "# voca size: 271 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "for category_num in range(18):\n",
    "    data = open('../vocabulary_all_1211/tokens{}_.txt'.format(category_num), 'r', encoding='utf-8')\n",
    "    token = data.readlines()\n",
    "    print(\"#\", category_num, \"번째 category (tokens):\", len(token))\n",
    "    \n",
    "    for i in range(len(token)):\n",
    "        token[i] = token[i].replace('\\n', '')\n",
    "    \n",
    "    setattr(mod, 'tokens{}'.format(category_num), token)\n",
    "    \n",
    "    #voca로도 저장해줍니다, 리스트 형태로 바꿔서 저장해줍시다\n",
    "    voca = list(set(token))\n",
    "    setattr(mod, 'voca{}'.format(category_num), voca)\n",
    "    print(\"# voca size:\", len(voca), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(category_sum)\n",
    "#print(sp_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "#print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
