{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 분석2. TF-IDF (least_word만 삭제한 token 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 각 카테고리의 데이터 개수(채용공고의 개수)를 가져와서 튜플형태로 저장한 뒤, 리스트에 넣는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 746), (1, 683), (2, 624), (3, 494), (4, 287), (5, 274), (6, 256), (7, 233), (8, 220), (9, 193), (10, 163), (11, 148), (12, 168), (13, 141), (14, 171), (15, 162), (16, 137), (17, 109)]\n"
     ]
    }
   ],
   "source": [
    "category_len = []\n",
    "\n",
    "for category_num in range(18):\n",
    "    data = open('../crawling_all_1211/category{0}.txt'.format(category_num), 'r', encoding = 'utf-8')\n",
    "    lines = data.readlines()  # 데이터 불러와서 line 단위로 넣는다\n",
    "    #print(len(lines))\n",
    "    tu = category_num, len(lines)\n",
    "    category_len.append(tu)\n",
    "    \n",
    "print(category_len)# ( 카테고리번호, 크롤링 데이터 개수 ) 모양의 튜플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 총 18개의 크롤링 데이터(클렌징까지 마침)를 불러와서, category_sum 리스트에 넣어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_num, data_num in category_len:\n",
    "    \n",
    "    data = open('../cleansing_all_1211/data_cleansing_all/cleansing_category{0}_list{1}.txt'.format(category_num, data_num), 'r', encoding = 'utf-8')\n",
    "    data = data.readlines()  # 데이터 불러와서 line 단위로 넣는다\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].replace('\\n', '')    # 각 문장마다 들어있는 줄바꿈 기호 없애주기\n",
    "    \n",
    "    data = ' '.join(data)\n",
    "    \n",
    "    ## 각 변수에 저장하자\n",
    "    globals()['category{}'.format(category_num)] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sum = []\n",
    "\n",
    "for category_num in range(18):\n",
    "    category = globals()['category{}'.format(category_num)]\n",
    "    category_sum.append(category)\n",
    "    \n",
    "len(category_sum) # 총 18개의 크롤링 데이터를 차례대로 category_sum 리스트에 넣어줬다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocabulary_all_1211 디렉토리 안의 tokens{}_.txt를 가져옵니다 (전처리가 완료된 토큰)\n",
    "\n",
    "# tokens{}: {}카테고리의 전처리 완료된 전체 토큰\n",
    "# voca{}: {}카테고리의 유니트 토큰\n",
    "\n",
    "# common_voca: 전체 데이터(1211) 공통등장 voca(토큰)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 번째 category (tokens): 39330\n",
      "# voca size: 1218 \n",
      "\n",
      "# 1 번째 category (tokens): 32265\n",
      "# voca size: 1121 \n",
      "\n",
      "# 2 번째 category (tokens): 31409\n",
      "# voca size: 1083 \n",
      "\n",
      "# 3 번째 category (tokens): 23563\n",
      "# voca size: 885 \n",
      "\n",
      "# 4 번째 category (tokens): 12172\n",
      "# voca size: 589 \n",
      "\n",
      "# 5 번째 category (tokens): 11449\n",
      "# voca size: 552 \n",
      "\n",
      "# 6 번째 category (tokens): 11239\n",
      "# voca size: 565 \n",
      "\n",
      "# 7 번째 category (tokens): 10772\n",
      "# voca size: 536 \n",
      "\n",
      "# 8 번째 category (tokens): 9931\n",
      "# voca size: 545 \n",
      "\n",
      "# 9 번째 category (tokens): 8665\n",
      "# voca size: 451 \n",
      "\n",
      "# 10 번째 category (tokens): 7375\n",
      "# voca size: 406 \n",
      "\n",
      "# 11 번째 category (tokens): 5730\n",
      "# voca size: 333 \n",
      "\n",
      "# 12 번째 category (tokens): 5838\n",
      "# voca size: 393 \n",
      "\n",
      "# 13 번째 category (tokens): 5838\n",
      "# voca size: 418 \n",
      "\n",
      "# 14 번째 category (tokens): 8058\n",
      "# voca size: 505 \n",
      "\n",
      "# 15 번째 category (tokens): 7181\n",
      "# voca size: 389 \n",
      "\n",
      "# 16 번째 category (tokens): 5625\n",
      "# voca size: 330 \n",
      "\n",
      "# 17 번째 category (tokens): 4639\n",
      "# voca size: 271 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "for category_num in range(18):\n",
    "    data = open('../vocabulary_all_1211/tokens{}_.txt'.format(category_num), 'r', encoding='utf-8')\n",
    "    token = data.readlines()\n",
    "    print(\"#\", category_num, \"번째 category (tokens):\", len(token))\n",
    "    \n",
    "    for i in range(len(token)):\n",
    "        token[i] = token[i].replace('\\n', '')\n",
    "    \n",
    "    setattr(mod, 'tokens{}'.format(category_num), token)\n",
    "    \n",
    "    #voca로도 저장해줍니다, 리스트 형태로 바꿔서 저장해줍시다\n",
    "    voca = list(set(token))\n",
    "    setattr(mod, 'voca{}'.format(category_num), voca)\n",
    "    print(\"# voca size:\", len(voca), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(category_sum)\n",
    "#print(sp_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "#print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 설문조사를 통해 선정한 5개의 카테고리의 TF-IDF를 구합니다\n",
    "# 파이썬개발자[6] 데이터엔지니어[7] 데이터사이언티스트[10] 머신러닝엔지니어[15] 빅데이터엔지니어[16]\n",
    "category = [6, 7, 10, 15, 16] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# category6(파이썬 개발자)의 voca의 TF-IDF수치 top15\n",
      "\n",
      "('경험', 0.5539156761450066)\n",
      "('개발', 0.41776235727577593)\n",
      "('python', 0.15644946578141405)\n",
      "('서비스', 0.12008553589708539)\n",
      "('관련', 0.11754851753306246)\n",
      "('경력', 0.11585717195704717)\n",
      "('운영', 0.10993746244099366)\n",
      "('사용', 0.10824611686497837)\n",
      "('aws', 0.09471535225685608)\n",
      "('이해', 0.08118458764873378)\n",
      "('경험자', 0.07949324207271849)\n",
      "('데이터', 0.07357353255666499)\n",
      "('설계', 0.06511680467658855)\n",
      "('java', 0.06427113188858091)\n",
      "('기반', 0.06173411352455798)\n"
     ]
    }
   ],
   "source": [
    "## Ex) category6에서 많이 등장했는데, 나머지 category에서 드물게 등장한 단어 == TF-IDF수치가 높다\n",
    "## category6의 voca의 TF-IDF수치를 확인해보자, >> 확인방법 : print(category_tfidf)\n",
    "\n",
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "for i, sent in enumerate(category_sum):\n",
    "    #print('====== document[%d]' % i, 'category', category[i], '======')\n",
    "    #print( [ (token, sp_matrix[i, word2id[token]]) for token in common_voca ] , '\\n')\n",
    "    a = [ (token, sp_matrix[i, word2id[token]]) for token in voca6 ]\n",
    "    setattr(mod, 'category{}_tfidf'.format(i), a) \n",
    "    \n",
    "    \n",
    "## category6의 voca의 TF-IDF수치 top10!!\n",
    "print('# category6(파이썬 개발자)의 voca의 TF-IDF수치 top15\\n')\n",
    "\n",
    "token = []\n",
    "tfidf = []\n",
    "for i in category6_tfidf:\n",
    "    token.append(i[0])\n",
    "    tfidf.append(i[1])\n",
    " \n",
    "\n",
    "a = []\n",
    "top = sorted(range(len(tfidf)),key= lambda i: tfidf[i])[-15:]  # tfidf수치 상위 10개의 인덱스 추출\n",
    "for i in category6_tfidf:\n",
    "    for idx in top:\n",
    "        if (i[0] == token[idx]):\n",
    "            a.append(i)\n",
    "p = []\n",
    "for i in a:\n",
    "    p.append(i[1])\n",
    "p = sorted(p, reverse=True)\n",
    "for i in p:\n",
    "    for j in a:\n",
    "        if (i == j[1]):\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# category7(데이터엔지니어)의 voca의 TF-IDF수치 top15\n",
      "\n",
      "('경험', 0.5080629459015115)\n",
      "('데이터', 0.33166496106771365)\n",
      "('개발', 0.2581658007202979)\n",
      "('운영', 0.1947727749206518)\n",
      "('경력', 0.1286235306079776)\n",
      "('분석', 0.12127361457323602)\n",
      "('관련', 0.12035487506889332)\n",
      "('구축', 0.10749252200809556)\n",
      "('python', 0.10565504299941017)\n",
      "('aws', 0.08911773192124162)\n",
      "('서비스', 0.08728025291255623)\n",
      "('경험자', 0.08452403439952813)\n",
      "('sql', 0.08268655539084274)\n",
      "('능력', 0.08268655539084274)\n",
      "('sql', 0.08268655539084274)\n",
      "('능력', 0.08268655539084274)\n",
      "('spark', 0.0755362293202715)\n"
     ]
    }
   ],
   "source": [
    "## category7에서 많이 등장했는데, 나머지 category에서 드물게 등장한 단어 == TF-IDF수치가 높다\n",
    "\n",
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "for i, sent in enumerate(category_sum):\n",
    "    #print('====== document[%d]' % i, 'category', category[i], '======')\n",
    "    #print( [ (token, sp_matrix[i, word2id[token]]) for token in common_voca ] , '\\n')\n",
    "    a = [ (token, sp_matrix[i, word2id[token]]) for token in voca7 ]\n",
    "    setattr(mod, 'category{}_tfidf'.format(i), a) \n",
    "    \n",
    "    \n",
    "## category7의 voca의 TF-IDF수치 top10!!\n",
    "print('# category7(데이터엔지니어)의 voca의 TF-IDF수치 top15\\n')\n",
    "\n",
    "token = []\n",
    "tfidf = []\n",
    "for i in category7_tfidf:\n",
    "    token.append(i[0])\n",
    "    tfidf.append(i[1])\n",
    " \n",
    "\n",
    "a = []\n",
    "top = sorted(range(len(tfidf)),key= lambda i: tfidf[i])[-15:]  # tfidf수치 상위 10개의 인덱스 추출\n",
    "for i in category7_tfidf:\n",
    "    for idx in top:\n",
    "        if (i[0] == token[idx]):\n",
    "            a.append(i)\n",
    "p = []\n",
    "for i in a:\n",
    "    p.append(i[1])\n",
    "p = sorted(p, reverse=True)\n",
    "for i in p:\n",
    "    for j in a:\n",
    "        if (i == j[1]):\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# category10(데이터 사이언티스트)의 voca의 TF-IDF수치 top15\n",
      "\n",
      "('경험', 0.44082565263920925)\n",
      "('데이터', 0.37821563240929257)\n",
      "('분석', 0.24021803680049664)\n",
      "('관련', 0.20060761583871262)\n",
      "('개발', 0.18527454965995752)\n",
      "('능력', 0.12010901840024832)\n",
      "('python', 0.11244248531087077)\n",
      "('머신러닝', 0.1121659831606466)\n",
      "('운영', 0.1022204411917007)\n",
      "('경력', 0.10094268567680444)\n",
      "('통계', 0.08530933930528052)\n",
      "('활용', 0.07794308640867179)\n",
      "('sql', 0.07538757537887927)\n",
      "('서비스', 0.074109819863983)\n",
      "('data', 0.07003569066257244)\n"
     ]
    }
   ],
   "source": [
    "## category10에서 많이 등장했는데, 나머지 category에서 드물게 등장한 단어 == TF-IDF수치가 높다\n",
    "\n",
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "for i, sent in enumerate(category_sum):\n",
    "    #print('====== document[%d]' % i, 'category', category[i], '======')\n",
    "    #print( [ (token, sp_matrix[i, word2id[token]]) for token in common_voca ] , '\\n')\n",
    "    a = [ (token, sp_matrix[i, word2id[token]]) for token in voca10 ]\n",
    "    setattr(mod, 'category{}_tfidf'.format(i), a) \n",
    "    \n",
    "    \n",
    "## category10의 voca의 TF-IDF수치 top10!!\n",
    "print('# category10(데이터 사이언티스트)의 voca의 TF-IDF수치 top15\\n')\n",
    "\n",
    "token = []\n",
    "tfidf = []\n",
    "for i in category10_tfidf:\n",
    "    token.append(i[0])\n",
    "    tfidf.append(i[1])\n",
    " \n",
    "\n",
    "a = []\n",
    "top = sorted(range(len(tfidf)),key= lambda i: tfidf[i])[-15:]  # tfidf수치 상위 10개의 인덱스 추출\n",
    "for i in category10_tfidf:\n",
    "    for idx in top:\n",
    "        if (i[0] == token[idx]):\n",
    "            a.append(i)\n",
    "p = []\n",
    "for i in a:\n",
    "    p.append(i[1])\n",
    "p = sorted(p, reverse=True)\n",
    "for i in p:\n",
    "    for j in a:\n",
    "        if (i == j[1]):\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복되는 값이 너무 많이 나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
