{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 data_stopwords_filtering 코드 저장용 파일입니다. 나중에 지워야합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(tokens6)\n",
    "a.plot(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca_6 = set(tokens6)\n",
    "#print(len(voca_6), voca_6)\n",
    "\n",
    "voca_7 = set(tokens7)\n",
    "voca_12 = set(tokens12)\n",
    "voca_13 = set(tokens13)\n",
    "voca_15 = set(tokens15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category6,7,12,13,15 공통 등장 토큰\n",
    "common_voca = voca_6 & voca_7 & voca_12 & voca_13 & voca_15\n",
    "#print(len(common_voca), common_voca)  # 총 557개\n",
    "\n",
    "common_voca = list(common_voca)\n",
    "\n",
    "encoded_6 = [common_voca.index(x) if x in common_voca else None for x in tokens6]\n",
    "# category6의 tokens가 common_voca에 있으면 숫자로 바꿔주고, 없으면 None붙여주기\n",
    "for x in zip(encoded_6, tokens6):\n",
    "    print(x)\n",
    "    \n",
    "# 나머지 category들도 똑같이 해주기~\n",
    "encoded_7 = [common_voca.index(x) if x in common_voca else None for x in tokens7]\n",
    "encoded_12 = [common_voca.index(x) if x in common_voca else None for x in tokens12]\n",
    "encoded_13 = [common_voca.index(x) if x in common_voca else None for x in tokens13]\n",
    "encoded_15 = [common_voca.index(x) if x in common_voca else None for x in tokens15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encoded_6))\n",
    "print(len(encoded_7))\n",
    "print(len(encoded_12))\n",
    "print(len(encoded_13))\n",
    "print(len(encoded_15))\n",
    "\n",
    "# 문서 길이를 동일하게 맞춰줘야 한다!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 방법1) 특정길이로 통일시키기 (문서 일부를 무시해야 한다.)\n",
    "# 우리는 특정 단어의 빈도수가 굉장히 중요하기 때문에 문서를 자를수는 없음\n",
    "# 심지어 데이터도 많은편이 아님!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 방법2) TF-IDF \n",
    "# 일단 한번 해봅시다\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_num, data_num in category_len:\n",
    "    \n",
    "    data = open('../data_cleansing_final/cleansing_category{0}_list{1}.txt'.format(category_num, data_num), 'r', encoding = 'utf-8')\n",
    "    data = data.readlines()  # 데이터 불러와서 line 단위로 넣는다\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].replace('\\n', '')    # 각 문장마다 들어있는 줄바꿈 기호 없애주기\n",
    "    \n",
    "    data = ' '.join(data)\n",
    "    \n",
    "    ## 각 변수에 저장하자\n",
    "    globals()['category{}'.format(category_num)] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = dict()\n",
    "token_dict['category6'] = category6\n",
    "token_dict['category7'] = category7\n",
    "token_dict['category12'] = category12\n",
    "token_dict['category13'] = category13\n",
    "token_dict['category15'] = category15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfs = tfidf.fit_transform(token_dict.values())\n",
    "#print(token_dict.values())\n",
    "#print(tfidf.vocabulary_)\n",
    "\n",
    "for x in tfidf.get_feature_names():\n",
    "    print(x, ': ', tfidf.vocabulary_[x])  # 몇번째 voca인지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(category6)):\n",
    "    text = category6[i]\n",
    "    token_dict['t{}'.format(i)] = text \n",
    "\n",
    "#print(tfidf.vocabulary_)\n",
    "\n",
    "for x in tfidf.get_feature_names():\n",
    "    print(x, ': ', tfidf.vocabulary_[x])  # 몇번째 voca인지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = category6[0]\n",
    "print(ttt, '\\n')\n",
    "resp = tfidf.transform([ttt])\n",
    "#print(resp)\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for col in resp.nonzero()[1]:\n",
    "    print(feature_names[col], ' - ', resp[0, col])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
