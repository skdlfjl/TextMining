{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## least_word_delete (3번 이하 등장 token삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14047\n"
     ]
    }
   ],
   "source": [
    "## 우선 tokens6 데이터로 테스트 해본다\n",
    "\n",
    "data = open('../stopwords_filtering/tokens6.txt', 'r', encoding='utf-8')\n",
    "data = data.readlines()\n",
    "print(len(data))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace('\\n', '')\n",
    "\n",
    "tokens6 = data\n",
    "#print(tokens6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens6 : [('경험', 1016), ('개발', 565), ('이해', 226), ('python', 212), ('서비스', 208), ('사용', 191), ('운영', 175), ('관련', 140), ('경력', 137), ('활용', 130)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print('tokens6 :', nltk.FreqDist(tokens6).most_common(10)) # 빈도수 top10 찍어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1814\n"
     ]
    }
   ],
   "source": [
    "#tokens6.count('경험')\n",
    "\n",
    "voca = set(tokens6)\n",
    "voca = list(voca)\n",
    "\n",
    "# 1~3번 등장한 단어를 찾아서 least_word 리스트에 추가\n",
    "least_word= []\n",
    "for i in voca:\n",
    "    if tokens6.count(i) < 4:\n",
    "        #print(i, tokens6.count(i))\n",
    "        least_word.append(i)\n",
    "        \n",
    "print(len(least_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2403 -> 589\n"
     ]
    }
   ],
   "source": [
    "# least_word 리스트에 포함되지 않은(4번 이상 등장한)token들만 tokens_ 변수에 저장\n",
    "tokens_ = [w for w in tokens6 if w not in least_word]\n",
    "\n",
    "print(len(set(tokens6)), '->', len(set(tokens_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 6 번째 category (tokens) : 14047\n",
      "# least_word 리스트 : 1814\n",
      "삭제 전 voca : ( 2403 ) -> 삭제 후 voca : ( 589 )\n",
      "삭제 후 tokens : 11487 \n",
      "\n",
      "# 7 번째 category (tokens) : 12229\n",
      "# least_word 리스트 : 1529\n",
      "삭제 전 voca : ( 2057 ) -> 삭제 후 voca : ( 528 )\n",
      "삭제 후 tokens : 10021 \n",
      "\n",
      "# 12 번째 category (tokens) : 8592\n",
      "# least_word 리스트 : 1278\n",
      "삭제 전 voca : ( 1675 ) -> 삭제 후 voca : ( 397 )\n",
      "삭제 후 tokens : 6679 \n",
      "\n",
      "# 13 번째 category (tokens) : 8049\n",
      "# least_word 리스트 : 1330\n",
      "삭제 전 voca : ( 1685 ) -> 삭제 후 voca : ( 355 )\n",
      "삭제 후 tokens : 6151 \n",
      "\n",
      "# 15 번째 category (tokens) : 6254\n",
      "# least_word 리스트 : 1040\n",
      "삭제 전 voca : ( 1361 ) -> 삭제 후 voca : ( 321 )\n",
      "삭제 후 tokens : 4694 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# category6,7,12,13,15 (11월08일 기준 크롤링 데이터) 에 적용해보기\n",
    "\n",
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "category = [6, 7, 12, 13, 15]\n",
    "\n",
    "for i in category:\n",
    "    data = open('../stopwords_filtering/tokens{}.txt'.format(i), 'r', encoding='utf-8')\n",
    "    token = data.readlines()\n",
    "    print(\"#\", i, \"번째 category (tokens) :\", len(token))\n",
    "    \n",
    "    for j in range(len(token)):\n",
    "        token[j] = token[j].replace('\\n', '')\n",
    "    \n",
    "    \n",
    "    voca = set(token)\n",
    "    voca = list(voca)\n",
    "\n",
    "    # 1~3번 등장한 단어를 찾아서 least_word 리스트에 추가\n",
    "    least_word= []\n",
    "    for j in voca:\n",
    "        if token.count(j) < 4:\n",
    "            least_word.append(j)\n",
    "    print(\"# least_word 리스트 :\", len(least_word))\n",
    "    \n",
    "    # least_word 리스트에 포함되지 않은(4번 이상 등장한)token들만 tokens_ 변수에 저장\n",
    "    token_ = [w for w in token if w not in least_word]\n",
    "    print(\"삭제 전 voca : (\", len(set(token)), \") ->\", \"삭제 후 voca : (\" , len(set(token_)), ')')\n",
    "    print(\"삭제 후 tokens :\" , len(token_), '\\n')\n",
    "\n",
    "    setattr(mod, 'tokens{}_'.format(i), token_)\n",
    "    \n",
    "#print(tokens7_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca6 = set(tokens6_)\n",
    "voca6 = list(voca6)\n",
    "\n",
    "a = {}\n",
    "for i in voca6:\n",
    "    a[i] = tokens6.count(i)\n",
    "    \n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 : {'제출', '실제', '학습', '도구', '전공자', '데이터', '오픈소스', '컨테이너', '서버', '파이프라인', 'tensorflow', '게임', '플랫폼', '경력', 'git', '영어', 'cd', '기초', '분산', '3년', '수행', '이용', '개선', '활용', '열정', '가능자', '설계', '보유자', '역량', '박사', '엔지니어링', '자료구조', '운영', 'hadoop', '경험', '기반', '통계학', '클라우드', '지식', '빅데이터', '필요', '경력자', '적극적', '도메인', '다양한', 'aws', '논문', '기본적', '관리', 'elasticsearch', '능력', '익숙', '추천', 'google', '방법', 'airflow', '소프트웨어', '처리', '성장', 'spark', 'computer', '진행', '러닝', 'java', '우대', '전공', '작성', 'machine', '원활한', '분석', '능숙', '관련', '조직', '협업', '1년', '이해', '내용', '문제', 'gcp', 'rdbms', '이해도', '머신러닝', '기계학습', 'ci', '프로그래밍', '기술', 'sql', '채용', '석사', '관심', '성능', 'cloud', '문제해결', '이론', '공학', 'deep', '환경', 'c++', '수집', '보유', '무관', '개발', '실시간', '소통', '비즈니스', 'data', '딥러닝', 'kafka', '1개', '연구', 'engineering', '모델링', '인프라', 'etl', '프레임워크', 'python', '유경험자', 'ai', 'ml', '언어', '통계', '커뮤니케이션', 'nlp', '유관', '기본', '구축', '사용', '로직', 'mysql', '프로젝트', '알고리즘', '서비스', '시각화', '컴퓨터공학', 'r', '해결', '구성', 'learning', '시스템', '금융', '학력', '컴퓨터', 'azure', '다양', '수학', '아키텍처', 'pytorch', '학사', '해당', '대용량', '2년', '배포', 'scala', 'system', '데이터베이스', '실무', '가능', '마케팅', 'c', '중급', '학위', '경험자', '모바', '분야', 'docker', '업무', '파이썬', 'experience', '검색', '구현', '모델', '경우', '5년', 'framework', '적용', 'nosql'}\n"
     ]
    }
   ],
   "source": [
    "voca6 = set(tokens6_)\n",
    "voca7 = set(tokens7_)\n",
    "voca12 = set(tokens12_)\n",
    "voca13 = set(tokens13_)\n",
    "voca15 = set(tokens15_)\n",
    "\n",
    "common_voca = voca6 & voca7 & voca12 & voca13 & voca15\n",
    "print(len(common_voca), ':', common_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for문을 이용해 least_word를 전부 삭제한 tokens_변수들을 전부 저장해줍시다\n",
    "for i in category:\n",
    "    tokens = globals()['tokens{}_'.format(i)]\n",
    "    line = '\\n'.join(tokens)\n",
    "    \n",
    "    f = open('tokens{}_.txt'.format(i), 'w')\n",
    "    f.write(line)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
