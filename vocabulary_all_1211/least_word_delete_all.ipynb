{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## least_word_delete (3번 이하 등장 token삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 번째 category (tokens) : 44052\n",
      "# least_word 리스트 : 3240\n",
      "삭제 전 voca : ( 4458 ) -> 삭제 후 voca : ( 1218 )\n",
      "삭제 후 tokens : 39330 \n",
      "\n",
      "# 1 번째 category (tokens) : 36401\n",
      "# least_word 리스트 : 2943\n",
      "삭제 전 voca : ( 4064 ) -> 삭제 후 voca : ( 1121 )\n",
      "삭제 후 tokens : 32265 \n",
      "\n",
      "# 2 번째 category (tokens) : 35375\n",
      "# least_word 리스트 : 2814\n",
      "삭제 전 voca : ( 3897 ) -> 삭제 후 voca : ( 1083 )\n",
      "삭제 후 tokens : 31409 \n",
      "\n",
      "# 3 번째 category (tokens) : 27067\n",
      "# least_word 리스트 : 2463\n",
      "삭제 전 voca : ( 3348 ) -> 삭제 후 voca : ( 885 )\n",
      "삭제 후 tokens : 23563 \n",
      "\n",
      "# 4 번째 category (tokens) : 14693\n",
      "# least_word 리스트 : 1694\n",
      "삭제 전 voca : ( 2283 ) -> 삭제 후 voca : ( 589 )\n",
      "삭제 후 tokens : 12172 \n",
      "\n",
      "# 5 번째 category (tokens) : 13800\n",
      "# least_word 리스트 : 1591\n",
      "삭제 전 voca : ( 2143 ) -> 삭제 후 voca : ( 552 )\n",
      "삭제 후 tokens : 11449 \n",
      "\n",
      "# 6 번째 category (tokens) : 13919\n",
      "# least_word 리스트 : 1916\n",
      "삭제 전 voca : ( 2481 ) -> 삭제 후 voca : ( 565 )\n",
      "삭제 후 tokens : 11239 \n",
      "\n",
      "# 7 번째 category (tokens) : 13131\n",
      "# least_word 리스트 : 1679\n",
      "삭제 전 voca : ( 2215 ) -> 삭제 후 voca : ( 536 )\n",
      "삭제 후 tokens : 10772 \n",
      "\n",
      "# 8 번째 category (tokens) : 12165\n",
      "# least_word 리스트 : 1561\n",
      "삭제 전 voca : ( 2106 ) -> 삭제 후 voca : ( 545 )\n",
      "삭제 후 tokens : 9931 \n",
      "\n",
      "# 9 번째 category (tokens) : 10840\n",
      "# least_word 리스트 : 1536\n",
      "삭제 전 voca : ( 1987 ) -> 삭제 후 voca : ( 451 )\n",
      "삭제 후 tokens : 8665 \n",
      "\n",
      "# 10 번째 category (tokens) : 9532\n",
      "# least_word 리스트 : 1487\n",
      "삭제 전 voca : ( 1893 ) -> 삭제 후 voca : ( 406 )\n",
      "삭제 후 tokens : 7375 \n",
      "\n",
      "# 11 번째 category (tokens) : 7643\n",
      "# least_word 리스트 : 1373\n",
      "삭제 전 voca : ( 1706 ) -> 삭제 후 voca : ( 333 )\n",
      "삭제 후 tokens : 5730 \n",
      "\n",
      "# 12 번째 category (tokens) : 7892\n",
      "# least_word 리스트 : 1461\n",
      "삭제 전 voca : ( 1854 ) -> 삭제 후 voca : ( 393 )\n",
      "삭제 후 tokens : 5838 \n",
      "\n",
      "# 13 번째 category (tokens) : 7931\n",
      "# least_word 리스트 : 1433\n",
      "삭제 전 voca : ( 1851 ) -> 삭제 후 voca : ( 418 )\n",
      "삭제 후 tokens : 5838 \n",
      "\n",
      "# 14 번째 category (tokens) : 10615\n",
      "# least_word 리스트 : 1749\n",
      "삭제 전 voca : ( 2254 ) -> 삭제 후 voca : ( 505 )\n",
      "삭제 후 tokens : 8058 \n",
      "\n",
      "# 15 번째 category (tokens) : 9272\n",
      "# least_word 리스트 : 1467\n",
      "삭제 전 voca : ( 1856 ) -> 삭제 후 voca : ( 389 )\n",
      "삭제 후 tokens : 7181 \n",
      "\n",
      "# 16 번째 category (tokens) : 7396\n",
      "# least_word 리스트 : 1223\n",
      "삭제 전 voca : ( 1553 ) -> 삭제 후 voca : ( 330 )\n",
      "삭제 후 tokens : 5625 \n",
      "\n",
      "# 17 번째 category (tokens) : 5996\n",
      "# least_word 리스트 : 935\n",
      "삭제 전 voca : ( 1206 ) -> 삭제 후 voca : ( 271 )\n",
      "삭제 후 tokens : 4639 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (12월 11일 기준 전체 크롤링 데이터)에 적용해보기\n",
    "\n",
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "category = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "\n",
    "for i in category:\n",
    "    data = open('../cleansing_all_1211/data_stopwords_filtering_all/tokens{}.txt'.format(i), 'r', encoding='utf-8')\n",
    "    token = data.readlines()\n",
    "    print(\"#\", i, \"번째 category (tokens) :\", len(token))\n",
    "    \n",
    "    for j in range(len(token)):\n",
    "        token[j] = token[j].replace('\\n', '')\n",
    "    \n",
    "    \n",
    "    voca = set(token)\n",
    "    voca = list(voca)\n",
    "\n",
    "    # 1~3번 등장한 단어를 찾아서 least_word 리스트에 추가\n",
    "    least_word= []\n",
    "    for j in voca:\n",
    "        if token.count(j) < 4:\n",
    "            least_word.append(j)\n",
    "    print(\"# least_word 리스트 :\", len(least_word))\n",
    "    \n",
    "    # least_word 리스트에 포함되지 않은(4번 이상 등장한)token들만 tokens_ 변수에 저장\n",
    "    token_ = [w for w in token if w not in least_word]\n",
    "    print(\"삭제 전 voca : (\", len(set(token)), \") ->\", \"삭제 후 voca : (\" , len(set(token_)), ')')\n",
    "    print(\"삭제 후 tokens :\" , len(token_), '\\n')\n",
    "\n",
    "    setattr(mod, 'tokens{}_'.format(i), token_)\n",
    "    \n",
    "#print(tokens7_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for문을 이용해 least_word를 전부 삭제한 tokens_변수들을 전부 저장해줍시다\n",
    "for i in category:\n",
    "    tokens = globals()['tokens{}_'.format(i)]\n",
    "    line = '\\n'.join(tokens)\n",
    "    \n",
    "    f = open('tokens{}_.txt'.format(i), 'w')\n",
    "    f.write(line)\n",
    "    f.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
